# Q-Learning z Gymnasium

Implementacja algorytmu Q-learning dla Å›rodowisk Gymnasium (OpenAI Gym) w ramach laboratorium ze sztucznej inteligencji.

## ğŸ“‹ Opis projektu

Projekt zawiera implementacje algorytmu Q-learning dla dwÃ³ch rÃ³Å¼nych Å›rodowisk:

1. **Blackjack-v1** - Agent uczÄ…cy siÄ™ gry w Blackjacka
2. **MountainCar-v0** - Agent uczÄ…cy siÄ™ prowadzenia samochodu na wzgÃ³rze

## ğŸš€ Technologie

- Python 3.x
- Gymnasium (OpenAI Gym)
- NumPy
- Matplotlib

## ğŸ“¦ Instalacja

```bash
pip install gymnasium numpy matplotlib
```

## ğŸ® Åšrodowiska

### Blackjack

Agent uczy siÄ™ optymalnej strategii gry w Blackjacka:
- **Liczba epizodÃ³w treningu**: 500,000
- **WspÃ³Å‚czynnik uczenia (alpha)**: 0.1
- **WspÃ³Å‚czynnik dyskontujÄ…cy (gamma)**: 0.99
- **Epsilon (eksploracja)**: 0.1

Wyniki testowe pokazujÄ…:
- LiczbÄ™ wygranych, przegranych i remisÃ³w
- SkutecznoÅ›Ä‡ agenta w %
- WizualizacjÄ™ Å›redniej ruchomej nagrÃ³d

### MountainCar

Agent uczy siÄ™ wypychania samochodu z doliny na wzgÃ³rze:
- **Liczba epizodÃ³w treningu**: 50,000
- **Dyskretyzacja stanu**: (18, 14) - pozycja i prÄ™dkoÅ›Ä‡
- Problem ciÄ…gÅ‚ego przestrzeni stanÃ³w rozwiÄ…zany przez dyskretyzacjÄ™

## ğŸ“Š Wyniki

Notebook generuje:
- Wykresy uczenia (Å›rednia ruchoma nagrÃ³d)
- Statystyki wydajnoÅ›ci agenta
- Wizualizacje wynikÃ³w testowych (dla Blackjacka)

## ğŸ¯ UÅ¼ycie

Uruchom notebook `lab4_arisc.ipynb` i wykonaj kolejne komÃ³rki:
1. Pierwsza komÃ³rka - podstawowa wersja Blackjacka
2. Druga komÃ³rka - rozszerzona wersja z dodatkowymi wizualizacjami
3. Trzecia komÃ³rka - MountainCar

## ğŸ“ˆ Parametry Q-learning

| Parametr | WartoÅ›Ä‡ | Opis |
|----------|---------|------|
| alpha (Î±) | 0.1 | WspÃ³Å‚czynnik uczenia |
| gamma (Î³) | 0.99 | WspÃ³Å‚czynnik dyskontujÄ…cy |
| epsilon (Îµ) | 0.1 | WspÃ³Å‚czynnik eksploracji |

## ğŸ” Algorytm

Implementacja klasycznego Q-learningu:

```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max(Q(s',a')) - Q(s,a)]
```

Gdzie:
- `s` - aktualny stan
- `a` - wybrana akcja
- `r` - nagroda
- `s'` - nastÄ™pny stan
- `Î±` - wspÃ³Å‚czynnik uczenia
- `Î³` - wspÃ³Å‚czynnik dyskontujÄ…cy

## ğŸ“ Uwagi

- Q-tabela reprezentowana jest jako `defaultdict` dla efektywnej pamiÄ™ci
- Agent uÅ¼ywa strategii Îµ-greedy (epsilon-greedy) do balansowania eksploracji i eksploatacji
- Dla MountainCar zastosowano dyskretyzacjÄ™ przestrzeni ciÄ…gÅ‚ej

## ğŸ‘¤ Autor

Projekt laboratoryjny - Sztuczna inteligencja

## ğŸ“„ Licencja

MIT License
